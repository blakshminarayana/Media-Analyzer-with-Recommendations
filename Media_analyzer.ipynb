{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1dyFyHAISte"
      },
      "outputs": [],
      "source": [
        "file = '/content/drive/MyDrive/ML TASK-20251013T190503Z-1-001.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile as zf\n",
        "data_zip = zf.ZipFile(file)\n",
        "data_zip.extractall()\n",
        "!ls"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z0b96Ha5I-AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics"
      ],
      "metadata": {
        "id": "vUtK_tfCJs7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install deepface"
      ],
      "metadata": {
        "id": "ABLpfGW6Js_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw\n",
        "import torch, cv2, numpy as np, os, re\n",
        "from ultralytics import YOLO\n",
        "from transformers import (\n",
        "    CLIPProcessor, CLIPModel,\n",
        "    VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer,\n",
        "    AutoModelForImageClassification, AutoImageProcessor\n",
        ")\n",
        "from deepface import DeepFace\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "iaUKqeDOJAnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bef976d5"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Running on:\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbe86d3f"
      },
      "source": [
        "print(\"Loading models... This may take a moment.\")\n",
        "yolo = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# CLIP for scene classification & explicit check\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_proc = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Captioning\n",
        "caption_id = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "cap_model = VisionEncoderDecoderModel.from_pretrained(caption_id).to(device)\n",
        "cap_feat = ViTImageProcessor.from_pretrained(caption_id)\n",
        "cap_tok = AutoTokenizer.from_pretrained(caption_id)\n",
        "\n",
        "# Dedicated NSFW detection\n",
        "print(\"Loading NSFW detection model...\")\n",
        "nsfw_model_id = \"Falconsai/nsfw_image_detection\"\n",
        "nsfw_model = AutoModelForImageClassification.from_pretrained(nsfw_model_id).to(device)\n",
        "nsfw_proc = AutoImageProcessor.from_pretrained(nsfw_model_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_objects(img):\n",
        "    res = yolo.predict(source=np.array(img), conf=0.35, verbose=False)\n",
        "    labels = set()\n",
        "    for r in res:\n",
        "        labels.update([r.names[int(c)] for c in r.boxes.cls.cpu().numpy()])\n",
        "    return list(labels)\n",
        "\n",
        "def classify_scene(img, class_list):\n",
        "    inp = clip_proc(text=class_list, images=img, return_tensors=\"pt\", padding=True).to(device)\n",
        "    out = clip_model(**inp)\n",
        "    probs = out.logits_per_image.softmax(dim=1)\n",
        "    idx = probs[0].topk(3).indices\n",
        "    return [class_list[i] for i in idx]\n",
        "\n",
        "def get_embedding(img):\n",
        "    with torch.no_grad():\n",
        "        inputs = clip_proc(images=img, return_tensors=\"pt\").to(device)\n",
        "        emb = clip_model.get_image_features(**inputs).cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "def face_analysis_if_person(img, objects_in_frame):\n",
        "    if \"person\" not in [obj.lower() for obj in objects_in_frame]:\n",
        "        return []\n",
        "    arr = cv2.cvtColor(np.array(img.convert(\"RGB\")), cv2.COLOR_RGB2BGR)\n",
        "    try:\n",
        "        faces = DeepFace.analyze(arr, actions=['age', 'gender'], enforce_detection=False)\n",
        "        return faces if isinstance(faces, list) else [faces]\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def generate_caption_and_tags(img):\n",
        "    pix = cap_feat(images=img, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    out_ids = cap_model.generate(pix, max_length=20, num_beams=4)\n",
        "    caption = cap_tok.decode(out_ids[0], skip_special_tokens=True).strip().capitalize()\n",
        "    words = re.findall(r\"\\b[a-zA-Z]{4,}\\b\", caption.lower())\n",
        "    tags = [f\"#{w}\" for w in dict.fromkeys(words)][:6]\n",
        "    return caption, tags\n",
        "\n",
        "def sample_video_frames(path, max_frames=5):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total == 0:\n",
        "        print(f\"‚ö†Ô∏è No frames found in video: {path}\")\n",
        "        cap.release()\n",
        "        return []\n",
        "    idxs = np.linspace(0, total - 1, max_frames).astype(int)\n",
        "    frames = []\n",
        "    for i in idxs:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, f = cap.read()\n",
        "        if ret:\n",
        "            frames.append(Image.fromarray(cv2.cvtColor(f, cv2.COLOR_BGR2RGB)))\n",
        "    cap.release()\n",
        "    if not frames:\n",
        "        print(f\"‚ö†Ô∏è Failed to read any frames from video: {path}\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "def detect_explicit_image(img):\n",
        "    \"\"\"Detects explicit (NSFW) content using specialized model.\"\"\"\n",
        "    inputs = nsfw_proc(images=img, return_tensors=\"pt\").to(device)\n",
        "    outputs = nsfw_model(**inputs)\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)[0]\n",
        "    labels = nsfw_model.config.id2label\n",
        "    top_label = labels[int(probs.argmax())]\n",
        "    confidence = float(probs.max())\n",
        "    return top_label, confidence\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    if isinstance(obj, np.float32) or isinstance(obj, np.float64):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_serializable(i) for i in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def get_media_recommendations(upload_emb, dataset_embeddings, dataset_paths, top_k=3):\n",
        "    sims = cosine_similarity(upload_emb.reshape(1, -1), dataset_embeddings)[0]\n",
        "    top_idx = sims.argsort()[::-1][:top_k]\n",
        "    return [dataset_paths[i] for i in top_idx]"
      ],
      "metadata": {
        "id": "14uXpQwXJK9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset_embeddings(dataset_folder):\n",
        "    dataset_paths = []\n",
        "    dataset_embeddings = []\n",
        "\n",
        "    images_folder = os.path.join(dataset_folder, \"Images\")\n",
        "    videos_folder = os.path.join(dataset_folder, \"Videos\")\n",
        "\n",
        "    print(f\"üîç Scanning dataset folders: {dataset_folder}\")\n",
        "    for root, _, files_in_dir in os.walk(images_folder):\n",
        "        for f in files_in_dir:\n",
        "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                path = os.path.join(root, f)\n",
        "                try:\n",
        "                    img = Image.open(path).convert(\"RGB\")\n",
        "                    emb = get_embedding(img)\n",
        "                    dataset_embeddings.append(emb)\n",
        "                    dataset_paths.append(path)\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Skipping image file {path} due to error: {e}\")\n",
        "\n",
        "\n",
        "    for root, _, files_in_dir in os.walk(videos_folder):\n",
        "        for f in files_in_dir:\n",
        "            if f.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
        "                path = os.path.join(root, f)\n",
        "                try:\n",
        "                    frames = sample_video_frames(path, max_frames=3)\n",
        "                    if frames:\n",
        "                        emb = get_embedding(frames[0])\n",
        "                        dataset_embeddings.append(emb)\n",
        "                        dataset_paths.append(path)\n",
        "                    else:\n",
        "                         print(f\"‚ö†Ô∏è Skipping video file {path} due to no valid frames.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Skipping video file {path} due to error: {e}\")\n",
        "\n",
        "\n",
        "    if not dataset_embeddings:\n",
        "        print(\"‚ùå No valid media files found in the dataset folder to compute embeddings.\")\n",
        "        return np.array([]), []\n",
        "\n",
        "    dataset_embeddings = np.vstack(dataset_embeddings)\n",
        "    dataset_embeddings = np.nan_to_num(dataset_embeddings, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    print(f\"‚úÖ Successfully computed embeddings for {len(dataset_paths)} files.\")\n",
        "    return dataset_embeddings, dataset_paths"
      ],
      "metadata": {
        "id": "bQWc2K9wKOCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_media(file_path, dataset_embeddings=None, dataset_paths=None):\n",
        "    class_list = [\"People\", \"Fashion\", \"Sports\", \"Food\", \"Nature\", \"Travel\",\n",
        "                  \"Animals\", \"Architecture\", \"Technology\", \"Vehicles\", \"Art\", \"Military\"]\n",
        "\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    frames = [Image.open(file_path).convert(\"RGB\")] if ext in [\".jpg\", \".jpeg\", \".png\"] else sample_video_frames(file_path, 5)\n",
        "\n",
        "    if not frames:\n",
        "        return {\"error\": \"No valid frames found for analysis.\"}\n",
        "\n",
        "    all_objs, all_cats, all_faces, embeds = [], [], [], []\n",
        "\n",
        "    for frame in frames:\n",
        "        objs_in_frame = detect_objects(frame)\n",
        "        all_objs += objs_in_frame\n",
        "        all_cats += classify_scene(frame, class_list)\n",
        "\n",
        "        frame_faces = face_analysis_if_person(frame, objs_in_frame)\n",
        "        if frame_faces:\n",
        "            all_faces = frame_faces  # only take first frame with faces\n",
        "            break\n",
        "\n",
        "        embeds.append(get_embedding(frame))\n",
        "\n",
        "    embed_mean = np.mean(embeds, axis=0) if embeds else None # Handle empty embeds case\n",
        "    if embed_mean is not None:\n",
        "        embed_mean = np.nan_to_num(embed_mean, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "    top_cats = list(dict.fromkeys(all_cats))[:3]\n",
        "\n",
        "    # Explicit detection (CLIP + NSFW)\n",
        "    explicit_texts = [\"safe photo\", \"explicit photo\"]\n",
        "    inp = clip_proc(text=explicit_texts, images=frames[0], return_tensors=\"pt\", padding=True).to(device)\n",
        "    out = clip_model(**inp)\n",
        "    probs = out.logits_per_image.softmax(dim=1)\n",
        "    clip_label = explicit_texts[int(probs.argmax())].capitalize()\n",
        "    clip_prob = float(probs[0][int(probs.argmax())])\n",
        "\n",
        "    nsfw_label, nsfw_conf = detect_explicit_image(frames[0])\n",
        "    nsfw_label = nsfw_label.capitalize()\n",
        "\n",
        "    if nsfw_label.lower() == \"nsfw\" or clip_label.lower() == \"explicit photo\":\n",
        "        explicit_label = \"Explicit Content Detected\"\n",
        "        explicit_prob = max(clip_prob, nsfw_conf)\n",
        "    else:\n",
        "        explicit_label = \"Safe Content\"\n",
        "        explicit_prob = max(clip_prob, nsfw_conf)\n",
        "\n",
        "    caption, tags = generate_caption_and_tags(frames[0])\n",
        "    engagement_score = int(1000 + 150 * len(set(all_objs)) + 60 * len(all_faces))\n",
        "\n",
        "    processed_faces = [convert_to_serializable(f) for f in all_faces]\n",
        "\n",
        "    result = {\n",
        "        \"primary_category\": top_cats[0] if top_cats else None,\n",
        "        \"top_categories\": top_cats,\n",
        "        \"objects\": list(set(all_objs)),\n",
        "        \"faces\": processed_faces,\n",
        "        \"explicit_label\": explicit_label,\n",
        "        \"explicit_probability\": round(explicit_prob, 3),\n",
        "        \"caption\": caption,\n",
        "        \"hashtags\": tags,\n",
        "        \"predicted_engagement\": engagement_score,\n",
        "        \"first_frame\": frames[0]\n",
        "    }\n",
        "\n",
        "    # Recommendations\n",
        "    if embed_mean is not None and dataset_embeddings is not None and dataset_embeddings.size > 0:\n",
        "        recommended_media = get_media_recommendations(embed_mean, dataset_embeddings, dataset_paths, top_k=3)\n",
        "        result[\"recommendations\"] = recommended_media\n",
        "    else:\n",
        "        result[\"recommendations\"] = []\n",
        "\n",
        "    return convert_to_serializable(result)"
      ],
      "metadata": {
        "id": "aOR_MAh2KUau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_results(image_path, results, is_video=False):\n",
        "    img = results.get('first_frame', None)\n",
        "    if img:\n",
        "        display(img)\n",
        "\n",
        "    html_content = f\"\"\"\n",
        "    <div style=\"border:2px solid #4CAF50; padding:15px; border-radius:10px; width:700px;\">\n",
        "        <h2 style=\"color:#4CAF50;\">Analysis Results</h2>\n",
        "        <p><b>Primary Category:</b> {results['primary_category']}</p>\n",
        "        <p><b>Top Categories:</b> {', '.join(results['top_categories'])}</p>\n",
        "        <p><b>Objects Detected:</b> {', '.join(results['objects'])}</p>\n",
        "        <p><b>Faces Detected:</b></p>\n",
        "        <ul>\n",
        "    \"\"\"\n",
        "\n",
        "    if results['faces']:\n",
        "        for face in results['faces']:\n",
        "            gender_val = face.get('gender', None)\n",
        "            if isinstance(gender_val, dict):\n",
        "                gender = max(gender_val, key=gender_val.get)\n",
        "            elif isinstance(gender_val, str):\n",
        "                gender = \"Man\" if \"male\" in gender_val.lower() else \"Woman\" if \"female\" in gender_val.lower() else \"N/A\"\n",
        "            elif isinstance(gender_val, (int, float)):\n",
        "                gender = \"Man\" if gender_val >= 0.5 else \"Woman\"\n",
        "            else:\n",
        "                gender = \"N/A\"\n",
        "            age = face.get('age', 'N/A')\n",
        "            html_content += f\"<li>Gender: {gender}, Age: {age}</li>\"\n",
        "    else:\n",
        "        html_content += \"<li>No people detected</li>\"\n",
        "\n",
        "    html_content += f\"\"\"\n",
        "        </ul>\n",
        "        <p><b>Caption:</b> {results['caption']}</p>\n",
        "        <p><b>Hashtags:</b> {' '.join(results['hashtags'])}</p>\n",
        "        <p><b>Explicit/Safe:</b> {results['explicit_label']} (Probability: {results['explicit_probability']})</p>\n",
        "    \"\"\"\n",
        "\n",
        "    if results.get(\"recommendations\"):\n",
        "        html_content += \"<p><b>Recommended Media:</b></p><ul>\"\n",
        "        for r in results[\"recommendations\"]:\n",
        "            html_content += f\"<li>{r}</li>\"\n",
        "        html_content += \"</ul>\"\n",
        "\n",
        "    html_content += \"</div>\"\n",
        "    display(HTML(html_content))"
      ],
      "metadata": {
        "id": "YXb0l-3pKZmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_folder = \"/content/DATASET IMAGES & VIDEOS FOR AI-ML TASK\"\n",
        "dataset_embeddings, dataset_paths = prepare_dataset_embeddings(dataset_folder)"
      ],
      "metadata": {
        "id": "h9RWXyhXKgWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üì§ Please upload an image or video file for analysis:\")\n",
        "uploaded = files.upload()\n",
        "for fname in uploaded.keys():\n",
        "    print(f\"\\nüîç Analyzing external media: {fname}\")\n",
        "    results = analyze_media(fname, dataset_embeddings, dataset_paths)\n",
        "    if \"error\" in results:\n",
        "        print(f\"‚ùå Error analyzing {fname}: {results['error']}\")\n",
        "    else:\n",
        "        display_results(fname, results)"
      ],
      "metadata": {
        "id": "gX-uO3CoKlNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6chRGYMjMDOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}